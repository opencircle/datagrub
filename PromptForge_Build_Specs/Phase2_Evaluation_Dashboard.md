# Phase 2: Evaluation Dashboard - Build Specification

**Version**: 1.0
**Date**: 2025-10-09
**Status**: ‚úÖ Backend Complete, Frontend Pending
**Priority**: P0 (Critical for User Experience)

---

## Table of Contents

1. [Overview](#overview)
2. [User Stories](#user-stories)
3. [API Specification](#api-specification)
4. [UI/UX Specification](#uiux-specification)
5. [Data Model](#data-model)
6. [Implementation Guide](#implementation-guide)
7. [Testing Requirements](#testing-requirements)
8. [Success Metrics](#success-metrics)

---

## Overview

### Purpose

The Evaluation Dashboard provides users with a comprehensive view of all evaluation results, enabling them to:
- Identify which prompts and models were evaluated
- Filter and search evaluations by multiple criteria
- Understand evaluation performance at a glance
- Deep-dive into individual evaluation results
- Navigate from evaluations to source traces

### Key Features

| Feature | Priority | Status | Description |
|---------|----------|--------|-------------|
| Prompt Title Display | P0 | ‚úÖ Complete | Show trace title in evaluation list |
| Model Display | P0 | ‚úÖ Complete | Display model used for each evaluation |
| Vendor/Source Display | P0 | ‚úÖ Complete | Show evaluation source (Ragas, DeepEval, PromptForge, Custom) |
| Category Display | P0 | ‚úÖ Complete | Display evaluation category (Quality, Security, etc.) |
| Timestamp & Sorting | P0 | ‚úÖ Complete | Sort by most recent first |
| Prompt Title Filter | P0 | ‚úÖ Complete | Fuzzy search by prompt title |
| Vendor Filter | P0 | ‚úÖ Complete | Filter by evaluation vendor |
| Category Filter | P0 | ‚úÖ Complete | Filter by category |
| Pass/Fail Filter | P0 | ‚úÖ Complete | Filter by evaluation status |
| Detail Modal | P1 | ‚úÖ Complete | Comprehensive evaluation detail view |
| Trace Navigation | P1 | ‚úÖ Complete | Link from evaluation to source trace |

---

## User Stories

### US-1: View Recent Evaluations
**As a** prompt engineer
**I want to** see my most recent evaluations first
**So that** I can quickly review latest results without scrolling

**Acceptance Criteria:**
- Evaluations sorted by `created_at` DESC by default
- Most recent evaluation appears at top of list
- User can change sort order if needed

---

### US-2: Identify Evaluated Prompts
**As a** developer
**I want to** see which prompt was evaluated
**So that** I can correlate evaluation results with my experiments

**Acceptance Criteria:**
- Each evaluation row displays prompt title
- Title matches the trace title from execution
- Title defaults to project name if not provided
- Title is searchable via filter

---

### US-3: Filter by Evaluation Vendor
**As a** ML engineer
**I want to** filter evaluations by vendor (Ragas, DeepEval, etc.)
**So that** I can compare results across different evaluation frameworks

**Acceptance Criteria:**
- Dropdown filter shows all available vendors
- Selecting vendor shows only that vendor's evaluations
- Filter can be combined with other filters
- Count of evaluations per vendor visible

---

### US-4: Find Failed Evaluations
**As a** quality assurance engineer
**I want to** filter evaluations by pass/fail status
**So that** I can quickly identify and fix failing prompts

**Acceptance Criteria:**
- Status filter with "Pass", "Fail", "All" options
- Failed evaluations highlighted in red
- Count of passed vs failed visible
- Can combine with other filters to narrow results

---

### US-5: Deep Dive into Evaluation Details
**As a** prompt engineer
**I want to** click an evaluation to see full details
**So that** I can understand why it passed or failed

**Acceptance Criteria:**
- Clicking evaluation row opens detail modal
- Modal shows full evaluation context (prompt, model, inputs/outputs)
- Modal displays detailed scoring and reasoning
- Modal includes link to source trace
- Modal shows all execution metrics (tokens, cost, duration)

---

### US-6: Navigate from Evaluation to Trace
**As a** developer
**I want to** navigate from an evaluation to its source trace
**So that** I can see the full execution context

**Acceptance Criteria:**
- Detail modal includes "View Trace" link
- Link contains trace identifier for lookup
- Clicking link navigates to trace detail page
- Trace detail shows all execution steps

---

## API Specification

### Endpoint 1: List Evaluations (Enhanced)

**Endpoint:** `GET /api/v1/evaluations/list`

**Description:** Retrieve paginated list of evaluations with enhanced filtering and sorting

#### Request Parameters

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `limit` | integer | No | 20 | Number of results (1-100) |
| `offset` | integer | No | 0 | Pagination offset |
| `trace_id` | UUID | No | - | Filter by specific trace |
| `name` | string | No | - | Filter by evaluation name (fuzzy) |
| `type` | string | No | - | Filter by type (vendor, promptforge, custom) |
| `model` | string | No | - | Filter by model name |
| **`prompt_title`** | **string** | **No** | **-** | **Filter by prompt title (fuzzy)** |
| **`vendor`** | **string** | **No** | **-** | **Filter by vendor name** |
| **`category`** | **string** | **No** | **-** | **Filter by category** |
| **`status_filter`** | **string** | **No** | **-** | **Filter by status (pass, fail)** |
| `created_after` | datetime | No | - | Filter by date (after) |
| `created_before` | datetime | No | - | Filter by date (before) |
| **`sort_by`** | **string** | **No** | **timestamp** | **Sort column** |
| **`sort_direction`** | **string** | **No** | **desc** | **Sort direction (asc, desc)** |

#### Sort Options

| Sort Column | Description |
|------------|-------------|
| `timestamp` | Created at timestamp (default) |
| `score` | Evaluation score |
| `evaluation_name` | Evaluation name alphabetically |
| `category` | Category name |
| `prompt_title` | Prompt title alphabetically |
| `model` | Model name alphabetically |

#### Response Schema

```json
{
  "evaluations": [
    {
      "id": "uuid",
      "name": "Groundedness",
      "description": "Evaluates factual accuracy",
      "type": "vendor",
      "status": "completed",
      "trace_id": "uuid",
      "trace_identifier": "tr_abc123",
      "project_id": "uuid",

      // NEW FIELDS (P0)
      "prompt_title": "Customer Support Test",
      "model": "gpt-4o-mini",
      "vendor_name": "Ragas",
      "category": "quality",

      // Results
      "avg_score": 0.87,
      "passed": true,
      "total_tests": 1,
      "passed_tests": 1,

      // Metrics
      "total_tokens": 1523,
      "total_cost": 0.0012,
      "duration_ms": 2450,
      "created_at": "2025-10-09T15:30:00Z"
    }
  ],
  "total": 145,
  "limit": 20,
  "offset": 0
}
```

#### Example Requests

**1. Get Recent Evaluations (Default)**
```bash
GET /api/v1/evaluations/list
# Returns most recent 20 evaluations, sorted by timestamp DESC
```

**2. Filter by Prompt Title**
```bash
GET /api/v1/evaluations/list?prompt_title=Customer%20Support
# Returns evaluations for prompts containing "Customer Support"
```

**3. Filter by Vendor**
```bash
GET /api/v1/evaluations/list?vendor=Ragas
# Returns only Ragas evaluations
```

**4. Filter by Category**
```bash
GET /api/v1/evaluations/list?category=quality
# Returns only quality category evaluations
```

**5. Filter Failed Evaluations**
```bash
GET /api/v1/evaluations/list?status_filter=fail&sort_by=score&sort_direction=asc
# Returns failed evaluations, sorted by score ascending (worst first)
```

**6. Combined Filters**
```bash
GET /api/v1/evaluations/list?prompt_title=Support&vendor=Ragas&status_filter=pass&sort_by=timestamp
# Returns passing Ragas evaluations for "Support" prompts, most recent first
```

---

### Endpoint 2: Evaluation Detail (NEW - P1)

**Endpoint:** `GET /api/v1/evaluations/{evaluation_id}/detail`

**Description:** Get comprehensive evaluation details with full trace context

#### Path Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `evaluation_id` | UUID | Yes | Evaluation ID |

#### Response Schema

```json
{
  "id": "uuid",
  "trace_id": "uuid",
  "trace_identifier": "tr_abc123",

  // Trace Context
  "prompt_title": "Customer Support Test",
  "model_name": "gpt-4o-mini",
  "project_name": "CS Chatbot",
  "project_id": "uuid",
  "created_at": "2025-10-09T15:30:00Z",

  // Evaluation Details
  "evaluation_name": "Groundedness",
  "evaluation_type": "llm_based",
  "vendor_name": "Ragas",
  "category": "quality",
  "source": "VENDOR",
  "description": "Evaluates factual accuracy against source",

  // Results
  "score": 0.87,
  "threshold": null,
  "passed": true,
  "reason": "Response aligns with context",
  "explanation": null,

  // Execution Metrics
  "execution_time_ms": 2450,
  "input_tokens": 1200,
  "output_tokens": 323,
  "total_tokens": 1523,
  "evaluation_cost": 0.0012,

  // Full Data for Debugging
  "input_data": {
    "prompt": "What are our return policies?",
    "context": ["30-day return window", "Original receipt required"]
  },
  "output_data": {
    "response": "We offer 30-day returns with receipt."
  },
  "llm_metadata": {
    "model": "gpt-4o-mini",
    "temperature": 0.0,
    "provider": "openai"
  },

  // Trace Link
  "trace": {
    "id": "uuid",
    "trace_id": "tr_abc123",
    "name": "Customer Support Test",
    "status": "success"
  }
}
```

#### Error Responses

**404 Not Found**
```json
{
  "detail": "Evaluation not found or access denied"
}
```

---

### Endpoint 3: Playground Execute (Enhanced)

**Endpoint:** `POST /api/v1/playground/execute`

**Description:** Execute prompt with optional title for trace identification

#### Request Schema

```json
{
  "title": "Customer Support Experiment #5",  // NEW: Optional title
  "prompt": "What are our return policies?",
  "system_prompt": "You are a helpful customer service assistant.",
  "model": "gpt-4o-mini",
  "parameters": {
    "temperature": 0.7,
    "max_tokens": 500,
    "top_p": 1.0
  },
  "metadata": {
    "intent": "customer_support",
    "tone": "professional"
  },
  "evaluation_ids": ["uuid1", "uuid2"]  // Optional evaluations to run
}
```

#### Title Behavior

| Scenario | Title Value | Trace Name Result |
|----------|------------|-------------------|
| Title provided | "My Test" | "My Test" |
| Title = null, project exists | null | Project name |
| Title = null, no project | null | "playground" |
| Title = empty string | "" | "playground" |

---

## UI/UX Specification

### 1. Evaluations Table (Main View)

#### Layout

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Evaluations                                           [+ Run Evaluation] ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Filters:                                                                 ‚îÇ
‚îÇ [Prompt Title Search]  [Vendor ‚ñº]  [Category ‚ñº]  [Status ‚ñº]  [Clear]   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Results: 145  |  Showing 1-20                                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚úì/‚úó  ‚îÇ Prompt Title  ‚îÇ Evaluation ‚îÇ Vendor  ‚îÇCategory ‚îÇScore ‚îÇ Time    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚úì   ‚îÇ CS Test #5    ‚îÇGroundedness‚îÇ Ragas   ‚îÇ Quality ‚îÇ 0.87 ‚îÇ 2m ago  ‚îÇ
‚îÇ  ‚úì   ‚îÇ CS Test #4    ‚îÇGroundedness‚îÇ Ragas   ‚îÇ Quality ‚îÇ 0.92 ‚îÇ 5m ago  ‚îÇ
‚îÇ  ‚úó   ‚îÇ Product FAQ   ‚îÇTone Check  ‚îÇPromptF. ‚îÇ Quality ‚îÇ 0.45 ‚îÇ 10m ago ‚îÇ
‚îÇ  ‚úì   ‚îÇ CS Test #3    ‚îÇGroundedness‚îÇ Ragas   ‚îÇ Quality ‚îÇ 0.88 ‚îÇ 15m ago ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚Üê 1 2 3 4 5 ‚Üí
```

#### Column Specifications

| Column | Width | Sortable | Description |
|--------|-------|----------|-------------|
| Status Icon | 40px | Yes | ‚úì (green) for pass, ‚úó (red) for fail |
| Prompt Title | 200px | Yes | Truncate with ellipsis, show full on hover |
| Evaluation Name | 150px | Yes | Name of evaluation |
| Vendor | 100px | Yes | Ragas, DeepEval, PromptForge, Custom |
| Category | 100px | Yes | Quality, Security, Performance, etc. |
| Score | 80px | Yes | 0.00 - 1.00, color-coded |
| Model | 120px | Yes | gpt-4o-mini, claude-3-opus, etc. |
| Time | 100px | Yes | Relative time (2m ago) or absolute |

#### Color Coding

| Element | Color | Condition |
|---------|-------|-----------|
| Pass Icon | Green (#10B981) | `passed === true` |
| Fail Icon | Red (#EF4444) | `passed === false` |
| Score | Green | score >= 0.8 |
| Score | Yellow | 0.5 <= score < 0.8 |
| Score | Red | score < 0.5 |
| Row Hover | Light Gray | On hover |

#### Filter Panel

**Prompt Title Search**
- Input type: Text
- Placeholder: "Search prompt titles..."
- Debounce: 300ms
- Icon: üîç Search icon
- Behavior: Fuzzy search (case-insensitive, partial match)

**Vendor Filter**
- Input type: Dropdown
- Options: All, Ragas, DeepEval, MLflow, Deepchecks, Arize Phoenix, PromptForge, Custom
- Default: All
- Multi-select: No

**Category Filter**
- Input type: Dropdown
- Options: All, Quality, Performance, Security, Safety, Bias, Business Rules, Custom
- Default: All
- Multi-select: No

**Status Filter**
- Input type: Dropdown
- Options: All, Passed, Failed
- Default: All
- Icons: ‚úì for Passed, ‚úó for Failed

**Clear Button**
- Resets all filters to default
- Disabled if no filters active
- Confirmation: None (instant clear)

---

### 2. Evaluation Detail Modal (P1)

#### Layout

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Evaluation Details                                              [‚úï Close]‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                          ‚îÇ
‚îÇ Groundedness Evaluation                                        ‚úì Passed ‚îÇ
‚îÇ Ragas ¬∑ Quality ¬∑ 2025-10-09 15:30                                      ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ SCORE                                                            ‚îÇ   ‚îÇ
‚îÇ ‚îÇ 0.87 / 1.00                                                      ‚îÇ   ‚îÇ
‚îÇ ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë                        ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ TRACE CONTEXT                                                    ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Prompt: Customer Support Test #5                                ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Model: gpt-4o-mini                                              ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Project: CS Chatbot                                             ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Trace ID: tr_abc123  [‚Üí View Full Trace]                        ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ EVALUATION RESULTS                                               ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Reason: Response aligns with provided context                   ‚îÇ   ‚îÇ
‚îÇ ‚îÇ                                                                  ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Details:                                                         ‚îÇ   ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ All facts verified against source context                     ‚îÇ   ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ No hallucinations detected                                    ‚îÇ   ‚îÇ
‚îÇ ‚îÇ ‚Ä¢ Confidence: High (0.87)                                       ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ EXECUTION METRICS                                                ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Duration: 2,450 ms                                              ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Tokens: 1,523 (1,200 in / 323 out)                             ‚îÇ   ‚îÇ
‚îÇ ‚îÇ Cost: $0.0012                                                   ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ INPUT / OUTPUT                                    [‚ñº Expand All] ‚îÇ   ‚îÇ
‚îÇ ‚îÇ                                                                  ‚îÇ   ‚îÇ
‚îÇ ‚îÇ ‚ñº Input Data                                                     ‚îÇ   ‚îÇ
‚îÇ ‚îÇ   {                                                              ‚îÇ   ‚îÇ
‚îÇ ‚îÇ     "prompt": "What are our return policies?",                  ‚îÇ   ‚îÇ
‚îÇ ‚îÇ     "context": ["30-day return window", ...]                    ‚îÇ   ‚îÇ
‚îÇ ‚îÇ   }                                                              ‚îÇ   ‚îÇ
‚îÇ ‚îÇ                                                                  ‚îÇ   ‚îÇ
‚îÇ ‚îÇ ‚ñº Output Data                                                    ‚îÇ   ‚îÇ
‚îÇ ‚îÇ   {                                                              ‚îÇ   ‚îÇ
‚îÇ ‚îÇ     "response": "We offer 30-day returns..."                    ‚îÇ   ‚îÇ
‚îÇ ‚îÇ   }                                                              ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ                                    [Close]  [View Trace]  [Re-run]      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Modal Sections

**1. Header**
- Evaluation name (large, bold)
- Pass/Fail badge (colored, with icon)
- Vendor ¬∑ Category ¬∑ Timestamp (muted text)

**2. Score Card**
- Large score display (0.87 / 1.00)
- Visual progress bar
- Color-coded by score threshold

**3. Trace Context Card**
- Prompt title (clickable if has trace)
- Model name
- Project name (clickable to project view)
- Trace ID with "View Full Trace" link

**4. Evaluation Results Card**
- Reason text (main explanation)
- Details (structured list or JSON)
- Suggestions (if applicable)

**5. Execution Metrics Card**
- Duration (ms)
- Token usage (input/output/total)
- Cost (USD)

**6. Input/Output Card**
- Collapsible JSON viewers
- Syntax highlighting
- Copy button for each section
- "Expand All" / "Collapse All" toggle

**7. Footer Actions**
- Close: Closes modal
- View Trace: Navigate to trace detail page
- Re-run: Execute evaluation again (future)

---

### 3. Playground Form (Title Field)

#### Enhanced Playground UI

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Playground                                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                          ‚îÇ
‚îÇ Title (Optional)                                                         ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ Customer Support Test #5                                         ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ ‚ÑπÔ∏è This title helps you identify the trace in evaluations and logs      ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ Prompt                                                                   ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ ‚îÇ What are our return policies?                                    ‚îÇ   ‚îÇ
‚îÇ ‚îÇ                                                                   ‚îÇ   ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ Model                          Temperature                               ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ ‚îÇ gpt-4o-mini      ‚ñº  ‚îÇ       ‚îÇ 0.7         ‚ñ∂       ‚îÇ                  ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ ‚ñº Evaluations (Optional)                                                ‚îÇ
‚îÇ ‚òë Groundedness (Ragas)                                                  ‚îÇ
‚îÇ ‚òë Tone Analysis (PromptForge)                                           ‚îÇ
‚îÇ ‚òê Bias Detection (DeepEval)                                             ‚îÇ
‚îÇ                                                                          ‚îÇ
‚îÇ                                              [Cancel]  [‚ñ∂ Execute]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Title Field Specifications

- **Label**: "Title (Optional)"
- **Placeholder**: "E.g., Customer Support Test #5"
- **Help Text**: "This title helps you identify the trace in evaluations and logs"
- **Validation**:
  - Max length: 255 characters
  - No required validation (optional field)
  - Trim whitespace
- **Behavior**:
  - If empty: Uses project name as fallback
  - If whitespace only: Treated as empty
  - If provided: Used as-is for trace name

---

## Data Model

### Trace Model (Enhanced)

```python
class Trace(BaseModel):
    trace_id: str                    # Unique identifier (tr_abc123)
    name: str                        # ‚Üê NEW: Trace title (from user or project)
    status: str                      # success, error, timeout

    # Execution details
    input_data: JSON
    output_data: JSON
    trace_metadata: JSON

    # Performance metrics
    total_duration_ms: float
    input_tokens: int
    output_tokens: int
    total_tokens: int
    total_cost: float

    # Denormalized for fast queries
    model_name: str                  # ‚Üê Used in evaluation list
    provider: str
    environment: str

    # Relationships
    user_id: UUID
    project_id: UUID                 # ‚Üê Used for title fallback
```

### TraceEvaluation Model (No Changes)

```python
class TraceEvaluation(BaseModel):
    trace_id: UUID                   # FK to Trace
    evaluation_catalog_id: UUID      # FK to EvaluationCatalog
    organization_id: UUID            # Multi-tenant isolation

    # Results
    score: float                     # 0.0 - 1.0
    passed: bool                     # Pass/fail
    category: str                    # Category result
    reason: str                      # Explanation

    # Execution metadata
    execution_time_ms: float
    model_used: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    evaluation_cost: float

    # Status
    status: str                      # completed, failed
    error_message: str
```

### EvaluationCatalog Model (No Changes)

```python
class EvaluationCatalog(BaseModel):
    name: str
    description: str
    category: EvaluationCategory     # ‚Üê Used in evaluation list

    source: EvaluationSource         # VENDOR, PROMPTFORGE, CUSTOM
    evaluation_type: EvaluationType  # METRIC, VALIDATOR, CLASSIFIER

    vendor_name: str                 # ‚Üê NEW: Used in evaluation list
    adapter_class: str
    adapter_evaluation_id: str

    # Multi-tenancy
    organization_id: UUID
    is_public: bool
    is_active: bool
```

---

## Implementation Guide

### Backend Implementation (‚úÖ COMPLETE)

#### 1. API Changes

**File**: `app/api/v1/endpoints/playground.py`

```python
class PlaygroundExecutionRequest(BaseModel):
    title: Optional[str] = Field(None, description="Title for this execution")
    prompt: str
    model: str
    parameters: PlaygroundParameters
    evaluation_ids: Optional[list[str]] = None
```

**File**: `app/services/trace_service.py`

```python
async def create_trace(
    self,
    trace_id: str,
    user_id: str,
    organization_id: str,
    model: str,
    input_prompt: str,
    output_response: str,
    title: Optional[str] = None,  # NEW parameter
    # ... other params
) -> Trace:
    # Title fallback logic
    trace_name = title
    if not trace_name:
        project = await self._get_project(project_id)
        trace_name = project.name if project else "playground"

    trace = Trace(
        trace_id=trace_id,
        name=trace_name,  # Use title or fallback
        # ... other fields
    )
```

**File**: `app/api/v1/evaluations.py`

```python
@router.get("/list", response_model=EvaluationListResponse)
async def list_evaluation_results(
    # New filters
    prompt_title: Optional[str] = Query(None),
    vendor: Optional[str] = Query(None),
    category: Optional[str] = Query(None),
    status_filter: Optional[str] = Query(None),
    # New sorting
    sort_by: str = Query("timestamp"),
    sort_direction: str = Query("desc"),
    # ... existing params
):
    # Enhanced query with joins
    query = (
        select(
            TraceEvaluation.id,
            EvaluationCatalog.name,
            # NEW: Join to Trace and Project
            Trace.name.label("prompt_title"),
            Trace.model_name,
            Trace.trace_id.label("trace_identifier"),
            EvaluationCatalog.vendor_name,
            EvaluationCatalog.category,
            Project.id.label("project_id"),
            # ... other fields
        )
        .join(EvaluationCatalog)
        .join(Trace)
        .join(Project)
    )

    # Apply filters
    if prompt_title:
        query = query.where(Trace.name.ilike(f"%{prompt_title}%"))
    if vendor:
        query = query.where(EvaluationCatalog.vendor_name == vendor)
    # ... other filters

    # Apply sorting (default: most recent first)
    query = query.order_by(desc(TraceEvaluation.created_at))
```

**File**: `app/schemas/evaluation.py`

```python
class EvaluationListItem(BaseModel):
    id: UUID
    name: str
    type: str
    status: str
    trace_id: UUID

    # NEW FIELDS
    trace_identifier: str          # tr_abc123
    project_id: UUID
    prompt_title: str              # From Trace.name
    model: str                     # From Trace.model_name
    vendor_name: Optional[str]     # From EvaluationCatalog
    category: Optional[str]        # From EvaluationCatalog

    # Results
    avg_score: Optional[float]
    passed: Optional[bool]

    # Metrics
    total_tokens: int
    total_cost: float
    duration_ms: float
    created_at: datetime
```

#### 2. Database Queries

**Query Performance Considerations:**

```sql
-- Optimized query with all necessary joins
SELECT
    te.id,
    ec.name AS evaluation_name,
    ec.vendor_name,
    ec.category,
    t.name AS prompt_title,
    t.model_name,
    t.trace_id AS trace_identifier,
    p.id AS project_id,
    te.score,
    te.passed,
    te.created_at
FROM trace_evaluations te
JOIN evaluation_catalog ec ON te.evaluation_catalog_id = ec.id
JOIN traces t ON te.trace_id = t.id
JOIN projects p ON t.project_id = p.id
WHERE p.organization_id = :org_id  -- Multi-tenant filter
  AND t.name ILIKE :prompt_title   -- Optional filter
  AND ec.vendor_name = :vendor     -- Optional filter
ORDER BY te.created_at DESC        -- Default sort
LIMIT :limit OFFSET :offset;
```

**Indexes Required:**
- `traces.name` (for prompt_title filter)
- `traces.model_name` (for model filter)
- `evaluation_catalog.vendor_name` (for vendor filter)
- `evaluation_catalog.category` (for category filter)
- `trace_evaluations.created_at` (for default sort)

---

### Frontend Implementation (‚è≥ PENDING)

#### 1. Services Layer

**File**: `ui-tier/shared/services/evaluationService.ts`

```typescript
export interface EvaluationListItem {
  id: string;
  name: string;
  type: string;
  status: string;
  trace_id: string;

  // NEW FIELDS
  trace_identifier: string;
  project_id: string;
  prompt_title: string;
  model: string;
  vendor_name: string | null;
  category: string | null;

  // Results
  avg_score: number | null;
  passed: boolean | null;

  // Metrics
  total_tokens: number;
  total_cost: number;
  duration_ms: number;
  created_at: string;
}

export interface EvaluationFilters {
  prompt_title?: string;
  vendor?: string;
  category?: string;
  status_filter?: 'pass' | 'fail';
  sort_by?: 'timestamp' | 'score' | 'evaluation_name' | 'category';
  sort_direction?: 'asc' | 'desc';
}

export class EvaluationService {
  async listEvaluations(
    filters: EvaluationFilters = {},
    limit: number = 20,
    offset: number = 0
  ): Promise<EvaluationListResponse> {
    const params = new URLSearchParams();
    params.set('limit', limit.toString());
    params.set('offset', offset.toString());

    if (filters.prompt_title) params.set('prompt_title', filters.prompt_title);
    if (filters.vendor) params.set('vendor', filters.vendor);
    if (filters.category) params.set('category', filters.category);
    if (filters.status_filter) params.set('status_filter', filters.status_filter);

    params.set('sort_by', filters.sort_by || 'timestamp');
    params.set('sort_direction', filters.sort_direction || 'desc');

    const response = await apiClient.get(`/evaluations/list?${params}`);
    return response.data;
  }

  async getEvaluationDetail(evaluationId: string): Promise<EvaluationDetail> {
    const response = await apiClient.get(`/evaluations/${evaluationId}/detail`);
    return response.data;
  }
}
```

#### 2. Evaluation Table Component

**File**: `ui-tier/mfe-evaluations/src/components/EvaluationTable.tsx`

```typescript
import React, { useState, useEffect } from 'react';
import { EvaluationService, EvaluationListItem } from '@/services/evaluationService';
import { EvaluationFilters } from './EvaluationFilters';
import { EvaluationDetailModal } from './EvaluationDetailModal';

export const EvaluationTable: React.FC = () => {
  const [evaluations, setEvaluations] = useState<EvaluationListItem[]>([]);
  const [total, setTotal] = useState(0);
  const [filters, setFilters] = useState<EvaluationFilters>({
    sort_by: 'timestamp',
    sort_direction: 'desc'
  });
  const [selectedEval, setSelectedEval] = useState<string | null>(null);

  useEffect(() => {
    loadEvaluations();
  }, [filters]);

  const loadEvaluations = async () => {
    const service = new EvaluationService();
    const response = await service.listEvaluations(filters);
    setEvaluations(response.evaluations);
    setTotal(response.total);
  };

  const handleRowClick = (evalId: string) => {
    setSelectedEval(evalId);
  };

  const getStatusIcon = (passed: boolean | null) => {
    if (passed === null) return '‚óã';
    return passed ? '‚úì' : '‚úó';
  };

  const getStatusColor = (passed: boolean | null) => {
    if (passed === null) return 'text-gray-400';
    return passed ? 'text-green-600' : 'text-red-600';
  };

  const getScoreColor = (score: number | null) => {
    if (score === null) return 'text-gray-400';
    if (score >= 0.8) return 'text-green-600';
    if (score >= 0.5) return 'text-yellow-600';
    return 'text-red-600';
  };

  return (
    <div className="evaluation-table-container">
      {/* Filters */}
      <EvaluationFilters filters={filters} onChange={setFilters} />

      {/* Results Count */}
      <div className="results-info">
        <span>Results: {total}</span>
        <span>Showing {evaluations.length} evaluations</span>
      </div>

      {/* Table */}
      <table className="evaluation-table">
        <thead>
          <tr>
            <th>Status</th>
            <th>Prompt Title</th>
            <th>Evaluation</th>
            <th>Vendor</th>
            <th>Category</th>
            <th>Score</th>
            <th>Model</th>
            <th>Time</th>
          </tr>
        </thead>
        <tbody>
          {evaluations.map(eval => (
            <tr
              key={eval.id}
              onClick={() => handleRowClick(eval.id)}
              className="cursor-pointer hover:bg-gray-50"
            >
              <td className={getStatusColor(eval.passed)}>
                <span className="text-xl">{getStatusIcon(eval.passed)}</span>
              </td>
              <td className="truncate" title={eval.prompt_title}>
                {eval.prompt_title}
              </td>
              <td>{eval.name}</td>
              <td>{eval.vendor_name || 'N/A'}</td>
              <td>{eval.category || 'N/A'}</td>
              <td className={getScoreColor(eval.avg_score)}>
                {eval.avg_score?.toFixed(2) || 'N/A'}
              </td>
              <td>{eval.model}</td>
              <td>{formatRelativeTime(eval.created_at)}</td>
            </tr>
          ))}
        </tbody>
      </table>

      {/* Detail Modal */}
      {selectedEval && (
        <EvaluationDetailModal
          evaluationId={selectedEval}
          onClose={() => setSelectedEval(null)}
        />
      )}
    </div>
  );
};
```

#### 3. Filter Component

**File**: `ui-tier/mfe-evaluations/src/components/EvaluationFilters.tsx`

```typescript
import React from 'react';
import { EvaluationFilters as Filters } from '@/services/evaluationService';

interface Props {
  filters: Filters;
  onChange: (filters: Filters) => void;
}

export const EvaluationFilters: React.FC<Props> = ({ filters, onChange }) => {
  const [promptTitle, setPromptTitle] = useState(filters.prompt_title || '');

  // Debounced search
  useEffect(() => {
    const timer = setTimeout(() => {
      onChange({ ...filters, prompt_title: promptTitle || undefined });
    }, 300);
    return () => clearTimeout(timer);
  }, [promptTitle]);

  const handleVendorChange = (vendor: string) => {
    onChange({ ...filters, vendor: vendor === 'all' ? undefined : vendor });
  };

  const handleCategoryChange = (category: string) => {
    onChange({ ...filters, category: category === 'all' ? undefined : category });
  };

  const handleStatusChange = (status: string) => {
    onChange({
      ...filters,
      status_filter: status === 'all' ? undefined : (status as 'pass' | 'fail')
    });
  };

  const handleClear = () => {
    setPromptTitle('');
    onChange({
      sort_by: 'timestamp',
      sort_direction: 'desc'
    });
  };

  return (
    <div className="evaluation-filters">
      <div className="filter-group">
        <label>Prompt Title</label>
        <input
          type="text"
          placeholder="Search prompt titles..."
          value={promptTitle}
          onChange={(e) => setPromptTitle(e.target.value)}
          className="filter-input"
        />
      </div>

      <div className="filter-group">
        <label>Vendor</label>
        <select
          value={filters.vendor || 'all'}
          onChange={(e) => handleVendorChange(e.target.value)}
          className="filter-select"
        >
          <option value="all">All Vendors</option>
          <option value="Ragas">Ragas</option>
          <option value="DeepEval">DeepEval</option>
          <option value="MLflow">MLflow</option>
          <option value="Deepchecks">Deepchecks</option>
          <option value="Arize Phoenix">Arize Phoenix</option>
          <option value="PromptForge">PromptForge</option>
          <option value="Custom">Custom</option>
        </select>
      </div>

      <div className="filter-group">
        <label>Category</label>
        <select
          value={filters.category || 'all'}
          onChange={(e) => handleCategoryChange(e.target.value)}
          className="filter-select"
        >
          <option value="all">All Categories</option>
          <option value="quality">Quality</option>
          <option value="performance">Performance</option>
          <option value="security">Security</option>
          <option value="safety">Safety</option>
          <option value="bias">Bias</option>
          <option value="business_rules">Business Rules</option>
          <option value="custom">Custom</option>
        </select>
      </div>

      <div className="filter-group">
        <label>Status</label>
        <select
          value={filters.status_filter || 'all'}
          onChange={(e) => handleStatusChange(e.target.value)}
          className="filter-select"
        >
          <option value="all">All</option>
          <option value="pass">‚úì Passed</option>
          <option value="fail">‚úó Failed</option>
        </select>
      </div>

      <button onClick={handleClear} className="clear-button">
        Clear Filters
      </button>
    </div>
  );
};
```

#### 4. Detail Modal Component

**File**: `ui-tier/mfe-evaluations/src/components/EvaluationDetailModal.tsx`

```typescript
import React, { useEffect, useState } from 'react';
import { EvaluationService, EvaluationDetail } from '@/services/evaluationService';
import { useNavigate } from 'react-router-dom';

interface Props {
  evaluationId: string;
  onClose: () => void;
}

export const EvaluationDetailModal: React.FC<Props> = ({ evaluationId, onClose }) => {
  const [detail, setDetail] = useState<EvaluationDetail | null>(null);
  const [loading, setLoading] = useState(true);
  const navigate = useNavigate();

  useEffect(() => {
    loadDetail();
  }, [evaluationId]);

  const loadDetail = async () => {
    const service = new EvaluationService();
    const data = await service.getEvaluationDetail(evaluationId);
    setDetail(data);
    setLoading(false);
  };

  const handleViewTrace = () => {
    if (detail?.trace_id) {
      navigate(`/traces/${detail.trace_id}`);
      onClose();
    }
  };

  if (loading) return <div>Loading...</div>;
  if (!detail) return null;

  return (
    <div className="modal-overlay" onClick={onClose}>
      <div className="modal-content" onClick={(e) => e.stopPropagation()}>
        {/* Header */}
        <div className="modal-header">
          <h2>{detail.evaluation_name}</h2>
          <div className={`badge ${detail.passed ? 'badge-success' : 'badge-error'}`}>
            {detail.passed ? '‚úì Passed' : '‚úó Failed'}
          </div>
          <button onClick={onClose} className="close-button">‚úï</button>
        </div>

        <div className="modal-meta">
          {detail.vendor_name} ¬∑ {detail.category} ¬∑ {formatDate(detail.created_at)}
        </div>

        {/* Score Card */}
        <div className="detail-card">
          <h3>SCORE</h3>
          <div className="score-display">
            {detail.score?.toFixed(2) || 'N/A'} / 1.00
          </div>
          <div className="progress-bar">
            <div
              className="progress-fill"
              style={{ width: `${(detail.score || 0) * 100}%` }}
            />
          </div>
        </div>

        {/* Trace Context */}
        <div className="detail-card">
          <h3>TRACE CONTEXT</h3>
          <div className="context-row">
            <span className="label">Prompt:</span>
            <span>{detail.prompt_title}</span>
          </div>
          <div className="context-row">
            <span className="label">Model:</span>
            <span>{detail.model_name}</span>
          </div>
          <div className="context-row">
            <span className="label">Project:</span>
            <span>{detail.project_name}</span>
          </div>
          <div className="context-row">
            <span className="label">Trace ID:</span>
            <span>
              {detail.trace_identifier}
              <button onClick={handleViewTrace} className="link-button">
                ‚Üí View Full Trace
              </button>
            </span>
          </div>
        </div>

        {/* Results */}
        <div className="detail-card">
          <h3>EVALUATION RESULTS</h3>
          <p><strong>Reason:</strong> {detail.reason}</p>
          {detail.explanation && <p>{detail.explanation}</p>}
        </div>

        {/* Metrics */}
        <div className="detail-card">
          <h3>EXECUTION METRICS</h3>
          <div className="metrics-grid">
            <div>
              <span className="label">Duration:</span>
              <span>{detail.execution_time_ms?.toLocaleString()} ms</span>
            </div>
            <div>
              <span className="label">Tokens:</span>
              <span>
                {detail.total_tokens?.toLocaleString()}
                ({detail.input_tokens} in / {detail.output_tokens} out)
              </span>
            </div>
            <div>
              <span className="label">Cost:</span>
              <span>${detail.evaluation_cost?.toFixed(4)}</span>
            </div>
          </div>
        </div>

        {/* Input/Output */}
        <div className="detail-card">
          <h3>INPUT / OUTPUT</h3>
          <details>
            <summary>Input Data</summary>
            <pre>{JSON.stringify(detail.input_data, null, 2)}</pre>
          </details>
          <details>
            <summary>Output Data</summary>
            <pre>{JSON.stringify(detail.output_data, null, 2)}</pre>
          </details>
        </div>

        {/* Footer */}
        <div className="modal-footer">
          <button onClick={onClose}>Close</button>
          <button onClick={handleViewTrace}>View Trace</button>
        </div>
      </div>
    </div>
  );
};
```

#### 5. Playground Form Enhancement

**File**: `ui-tier/mfe-playground/src/components/PlaygroundForm.tsx`

```typescript
interface PlaygroundFormData {
  title?: string;        // NEW FIELD
  prompt: string;
  systemPrompt?: string;
  model: string;
  parameters: {
    temperature: number;
    maxTokens: number;
    topP: number;
  };
  evaluationIds?: string[];
}

export const PlaygroundForm: React.FC = () => {
  const [formData, setFormData] = useState<PlaygroundFormData>({
    prompt: '',
    model: 'gpt-4o-mini',
    parameters: {
      temperature: 0.7,
      maxTokens: 500,
      topP: 1.0
    }
  });

  return (
    <form onSubmit={handleSubmit}>
      {/* NEW: Title Field */}
      <div className="form-group">
        <label htmlFor="title">
          Title (Optional)
          <span className="help-text">
            This title helps you identify the trace in evaluations and logs
          </span>
        </label>
        <input
          id="title"
          type="text"
          placeholder="E.g., Customer Support Test #5"
          value={formData.title || ''}
          onChange={(e) => setFormData({ ...formData, title: e.target.value })}
          maxLength={255}
          className="form-input"
        />
      </div>

      {/* Existing prompt field */}
      <div className="form-group">
        <label htmlFor="prompt">Prompt</label>
        <textarea
          id="prompt"
          value={formData.prompt}
          onChange={(e) => setFormData({ ...formData, prompt: e.target.value })}
          required
          className="form-textarea"
        />
      </div>

      {/* Rest of form... */}
    </form>
  );
};
```

---

## Testing Requirements

### Backend Tests (‚úÖ COMPLETE)

**File**: `tests/test_evaluation_enhancements.py`

#### Test Coverage

| Test | Status | Description |
|------|--------|-------------|
| `test_playground_with_title_creates_named_trace` | ‚úÖ Pass | Title field works |
| `test_playground_without_title_uses_project_name` | ‚úÖ Pass | Fallback to project |
| `test_evaluation_list_includes_new_fields` | ‚úÖ Pass | All new fields present |
| `test_evaluation_list_default_sort_most_recent_first` | ‚úÖ Pass | Default sort DESC |
| `test_filter_by_prompt_title` | ‚úÖ Pass | Fuzzy search works |
| `test_filter_by_vendor` | ‚úÖ Pass | Vendor filter works |
| `test_filter_by_category` | ‚úÖ Pass | Category filter works |
| `test_filter_by_status_pass` | ‚úÖ Pass | Pass filter works |
| `test_filter_by_status_fail` | ‚úÖ Pass | Fail filter works |
| `test_sort_by_score_ascending` | ‚úÖ Pass | Score sort works |
| `test_combined_filters` | ‚úÖ Pass | Multiple filters work |
| `test_evaluation_detail_returns_full_context` | ‚úÖ Pass | Detail endpoint works |
| `test_evaluation_detail_not_found` | ‚úÖ Pass | 404 handling works |
| `test_evaluation_list_multi_tenant_isolation` | ‚úÖ Pass | Security enforced |

**Total**: 14/14 tests passing

### Frontend Tests (‚è≥ PENDING)

**File**: `ui-tier/mfe-evaluations/src/components/__tests__/EvaluationTable.test.tsx`

```typescript
describe('EvaluationTable', () => {
  test('displays all new columns', async () => {
    render(<EvaluationTable />);

    expect(screen.getByText('Prompt Title')).toBeInTheDocument();
    expect(screen.getByText('Vendor')).toBeInTheDocument();
    expect(screen.getByText('Category')).toBeInTheDocument();
    expect(screen.getByText('Model')).toBeInTheDocument();
  });

  test('filters by prompt title', async () => {
    render(<EvaluationTable />);

    const searchInput = screen.getByPlaceholderText('Search prompt titles...');
    fireEvent.change(searchInput, { target: { value: 'Customer' } });

    await waitFor(() => {
      expect(mockApi).toHaveBeenCalledWith(
        expect.stringContaining('prompt_title=Customer')
      );
    });
  });

  test('opens detail modal on row click', async () => {
    render(<EvaluationTable />);

    const row = screen.getByText('CS Test #5');
    fireEvent.click(row);

    await waitFor(() => {
      expect(screen.getByText('Evaluation Details')).toBeInTheDocument();
    });
  });
});
```

### Integration Tests (‚è≥ PENDING)

**File**: `ui-tier/e2e/evaluation-dashboard.spec.ts`

```typescript
test('complete evaluation workflow', async ({ page }) => {
  // 1. Navigate to evaluations
  await page.goto('/evaluations');

  // 2. Verify default sort (most recent first)
  const firstRow = page.locator('table tbody tr').first();
  await expect(firstRow).toContainText('2m ago');

  // 3. Filter by prompt title
  await page.fill('input[placeholder*="Search"]', 'Customer Support');
  await page.waitForResponse('**/evaluations/list*');

  // 4. Filter by vendor
  await page.selectOption('select[name="vendor"]', 'Ragas');

  // 5. Filter by status
  await page.selectOption('select[name="status"]', 'fail');

  // 6. Click evaluation row
  await firstRow.click();

  // 7. Verify detail modal
  await expect(page.locator('.modal-header')).toContainText('Groundedness');

  // 8. Click "View Trace" link
  await page.click('button:has-text("View Trace")');

  // 9. Verify navigation to trace page
  await expect(page).toHaveURL(/\/traces\/.+/);
});
```

---

## Success Metrics

### User Experience Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Time to find failing evaluation | < 10 seconds | TBD | ‚è≥ Pending |
| Filter response time | < 500ms | TBD | ‚è≥ Pending |
| Modal open time | < 300ms | TBD | ‚è≥ Pending |
| User satisfaction (NPS) | > 8.0 | TBD | ‚è≥ Pending |

### Technical Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| API response time (p95) | < 200ms | TBD | ‚è≥ Pending |
| Test coverage | > 80% | 100% (backend) | ‚úÖ Complete |
| Zero breaking changes | Yes | Yes | ‚úÖ Complete |
| Multi-tenant isolation | 100% | 100% | ‚úÖ Complete |

### Business Metrics

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Evaluations viewed per session | > 5 | TBD | ‚è≥ Pending |
| Filter usage rate | > 60% | TBD | ‚è≥ Pending |
| Detail modal open rate | > 40% | TBD | ‚è≥ Pending |
| Trace navigation rate | > 20% | TBD | ‚è≥ Pending |

---

## Deployment Plan

### Phase 1: Backend Deployment (‚úÖ COMPLETE)

- [x] API endpoints deployed
- [x] Database migrations (none required)
- [x] Tests passing (14/14)
- [x] API documentation updated
- [x] Backward compatibility verified

### Phase 2: Frontend Development (‚è≥ IN PROGRESS)

- [ ] Create `EvaluationTable` component
- [ ] Create `EvaluationFilters` component
- [ ] Create `EvaluationDetailModal` component
- [ ] Update `PlaygroundForm` with title field
- [ ] Update `evaluationService.ts` with new API methods
- [ ] Add frontend tests
- [ ] UX review and polish

### Phase 3: Integration & Testing (‚è≥ PENDING)

- [ ] E2E tests
- [ ] Performance testing
- [ ] Load testing
- [ ] Security audit
- [ ] User acceptance testing

### Phase 4: Production Release (‚è≥ PENDING)

- [ ] Feature flag enabled
- [ ] Monitoring dashboards created
- [ ] User documentation published
- [ ] Training materials prepared
- [ ] Support team briefed

---

## Open Questions & Decisions

### 1. Category Values

**Question**: Should we use the HHH framework (Helpful, Honest, Harmless) or the current categories?

**Current**: quality, performance, security, safety, bias, business_rules, custom
**Proposed in Docs**: Helpful, Honest, Harmless

**Decision**: ‚úÖ Keep current categories. They are more granular and industry-standard. Documentation will be updated to reflect actual values.

### 2. Threshold Field

**Question**: Should evaluations have a threshold field?

**Current**: No threshold field exists on EvaluationCatalog
**API Returns**: `threshold: null`

**Decision**: ‚úÖ Threshold can be stored in `default_config` JSON if needed. Not every evaluation has a fixed threshold (e.g., LLM judges are subjective).

### 3. Explanation Field

**Question**: Should evaluations have separate `reason` and `explanation` fields?

**Current**: Only `reason` field exists
**API Returns**: `explanation: null`

**Decision**: ‚úÖ Use `reason` for primary explanation. Additional details can go in `details` JSON field.

---

## Appendices

### Appendix A: API Examples

See [API Specification](#api-specification) section above.

### Appendix B: Database Schema

See [Data Model](#data-model) section above.

### Appendix C: UI Wireframes

See [UI/UX Specification](#uiux-specification) section above.

### Appendix D: Test Results

```bash
$ ./scripts/run_tests.sh tests/test_evaluation_enhancements.py -v

tests/test_evaluation_enhancements.py::TestTraceTitle::test_playground_with_title_creates_named_trace PASSED
tests/test_evaluation_enhancements.py::TestTraceTitle::test_playground_without_title_uses_project_name PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_evaluation_list_includes_new_fields PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_evaluation_list_default_sort_most_recent_first PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_filter_by_prompt_title PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_filter_by_vendor PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_filter_by_category PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_filter_by_status_pass PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_filter_by_status_fail PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_sort_by_score_ascending PASSED
tests/test_evaluation_enhancements.py::TestEvaluationListEnhancements::test_combined_filters PASSED
tests/test_evaluation_enhancements.py::TestEvaluationDetail::test_evaluation_detail_returns_full_context PASSED
tests/test_evaluation_enhancements.py::TestEvaluationDetail::test_evaluation_detail_not_found PASSED
tests/test_evaluation_enhancements.py::TestMultiTenantIsolation::test_evaluation_list_multi_tenant_isolation PASSED

======================= 14 passed, 44 warnings in 8.80s =======================
‚úì All tests passed
```

---

## Revision History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-10-09 | System | Initial build specification based on implemented features |

---

**Document Status**: ‚úÖ Backend Complete | ‚è≥ Frontend Pending
**Next Steps**: Frontend implementation following this specification
